{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYHhT8iX9AsE",
        "outputId": "e136c783-c96c-4f91-8584-e273ff24369e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install & import\n",
        "!pip install pandas scikit-learn joblib\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import classification_report, mean_squared_error\n",
        "import kagglehub\n",
        "import polars as pl\n",
        "import joblib\n",
        "import shutil\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LoadSetDataset:\n",
        "    def __init__(self):\n",
        "      self.path = kagglehub.dataset_download(\"nikhil1e9/loan-default\")\n",
        "      self.destination_path = '/content/kaggle/'\n",
        "      self.df=None\n",
        "\n",
        "    @staticmethod\n",
        "    def load_dataset(self):\n",
        "      source_path = self.path\n",
        "      destination_path = self.destination_path\n",
        "      shutil.copytree(source_path, destination_path, dirs_exist_ok=True)\n",
        "      print(\"Path to dataset files:\", self.path, \"\\n Copied Path of Dataset:\", destination_path)\n",
        "      return f\"{destination_path}/Loan_default.csv\"\n",
        "\n",
        "    def read_dataset(self):\n",
        "      self.df = pl.read_csv(self.load_dataset(self))\n",
        "      print(self.df.head())\n",
        "      return self.df"
      ],
      "metadata": {
        "id": "l6CWplEazfUb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreprocessDataset(LoadSetDataset):\n",
        "    def __init__(self, n):\n",
        "      super().__init__()\n",
        "      self.df = self.read_dataset()\n",
        "      self.n = n\n",
        "\n",
        "    def cleanup_imputation(self):\n",
        "      # Drop rows with missing values in specified columns\n",
        "      self.df = self.df.drop_nulls(subset=['Default','CreditScore','LoanAmount'])  # minimal cleanup\n",
        "      print(self.df.shape)\n",
        "      return self.df\n",
        "\n",
        "    def set_range(self):\n",
        "      self.df = self.df.head(self.n)\n",
        "      print(self.df.shape)\n",
        "      return self.df\n",
        "\n",
        "    def __del__(self):\n",
        "      del self.df"
      ],
      "metadata": {
        "id": "0NiD_zATTxQ0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ldst=PreprocessDataset(1247)\n",
        "ldst.cleanup_imputation()\n",
        "df=ldst.set_range()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoePlvJvU-yh",
        "outputId": "429925bc-7ded-4b6c-888b-339ad4598759"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/nikhil1e9/loan-default?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7.86M/7.86M [00:00<00:00, 108MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/nikhil1e9/loan-default/versions/2 \n",
            " Copied Path of Dataset: /content/kaggle/\n",
            "shape: (5, 18)\n",
            "┌────────────┬─────┬────────┬────────────┬───┬───────────────┬─────────────┬─────────────┬─────────┐\n",
            "│ LoanID     ┆ Age ┆ Income ┆ LoanAmount ┆ … ┆ HasDependents ┆ LoanPurpose ┆ HasCoSigner ┆ Default │\n",
            "│ ---        ┆ --- ┆ ---    ┆ ---        ┆   ┆ ---           ┆ ---         ┆ ---         ┆ ---     │\n",
            "│ str        ┆ i64 ┆ i64    ┆ i64        ┆   ┆ str           ┆ str         ┆ str         ┆ i64     │\n",
            "╞════════════╪═════╪════════╪════════════╪═══╪═══════════════╪═════════════╪═════════════╪═════════╡\n",
            "│ I38PQUQS96 ┆ 56  ┆ 85994  ┆ 50587      ┆ … ┆ Yes           ┆ Other       ┆ Yes         ┆ 0       │\n",
            "│ HPSK72WA7R ┆ 69  ┆ 50432  ┆ 124440     ┆ … ┆ No            ┆ Other       ┆ Yes         ┆ 0       │\n",
            "│ C1OZ6DPJ8Y ┆ 46  ┆ 84208  ┆ 129188     ┆ … ┆ Yes           ┆ Auto        ┆ No          ┆ 1       │\n",
            "│ V2KKSFM3UN ┆ 32  ┆ 31713  ┆ 44799      ┆ … ┆ No            ┆ Business    ┆ No          ┆ 0       │\n",
            "│ EY08JDHTZP ┆ 60  ┆ 20437  ┆ 9139       ┆ … ┆ Yes           ┆ Auto        ┆ No          ┆ 0       │\n",
            "└────────────┴─────┴────────┴────────────┴───┴───────────────┴─────────────┴─────────────┴─────────┘\n",
            "(255347, 18)\n",
            "(1247, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class splitDataset:\n",
        "  X_train, y_clf_train, y_reg_train = None, None, None\n",
        "  X_test, y_clf_test, y_reg_test = None, None, None\n",
        "  def __init__(self, df):\n",
        "    self.df = df\n",
        "    self.X = None\n",
        "    self.y_clf = None\n",
        "    self.y_reg = None\n",
        "\n",
        "  def X_y_split(self):\n",
        "    # One-hot encoding for categorical features\n",
        "    self.X = self.df.to_pandas().drop(['Default','LoanAmount'], axis=1)\n",
        "    self.X = pl.from_pandas(pd.get_dummies(self.X))\n",
        "    # Define targets\n",
        "    self.y_clf = self.df['Default']\n",
        "    self.y_reg = self.df['LoanAmount'].fill_null(0)  # regression target\n",
        "    return self.X, self.y_clf, self.y_reg\n",
        "\n",
        "  @classmethod\n",
        "  def train_test_split(cls, X, y_clf, y_reg):\n",
        "    X_pd = X.to_pandas()\n",
        "    y_clf_pd = y_clf.to_pandas()\n",
        "    y_reg_pd = y_reg.to_pandas()\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_clf_train, y_clf_test, y_reg_train, y_reg_test = train_test_split(\n",
        "        X_pd, y_clf_pd, y_reg_pd, test_size=0.2, random_state=42\n",
        "    )\n",
        "    return X_train, X_test, y_clf_train, y_clf_test, y_reg_train, y_reg_test\n",
        "\n",
        "  def process_split(self):\n",
        "    X, y_clf, y_reg = self.X_y_split()\n",
        "    X_train, X_test, y_clf_train, y_clf_test, y_reg_train, y_reg_test = self.train_test_split(X, y_clf, y_reg)\n",
        "    return X_train, X_test, y_clf_train, y_clf_test, y_reg_train, y_reg_test\n",
        "\n",
        "  def __del__(self):\n",
        "    del self.df"
      ],
      "metadata": {
        "id": "u09D5GhgWOZA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spd=splitDataset(df)\n",
        "X_train, X_test, y_clf_train, y_clf_test, y_reg_train, y_reg_test = spd.process_split()\n",
        "print(X_train.shape, X_test.shape, y_clf_train.shape, y_clf_test.shape, y_reg_train.shape, y_reg_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6CNF1xOi3K9",
        "outputId": "e3327b03-fa8e-4a35-8be2-0a7146cd917e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(997, 1277) (250, 1277) (997,) (250,) (997,) (250,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "class Model(ABC):\n",
        "  @abstractmethod\n",
        "  def __init__(self, X_train, X_test, y_clf_train, y_reg_train, y_clf_test, y_reg_test):\n",
        "      self.X_train = X_train\n",
        "      self.X_test = X_test\n",
        "      self.y_clf_train = y_clf_train\n",
        "      self.y_reg_train = y_reg_train\n",
        "      self.y_clf_test = y_clf_test\n",
        "      self.y_reg_test = y_reg_test\n",
        "\n",
        "  @abstractmethod\n",
        "  def process_model(self):\n",
        "      pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def train_model(self):\n",
        "      pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def evaluate_model(self):\n",
        "      pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def run_model(self):\n",
        "      pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def __del__(self):\n",
        "      pass"
      ],
      "metadata": {
        "id": "iru-ADI3jI-X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class regression_model(Model):\n",
        "  def __init__(self, X_train, X_test, y_clf_train, y_reg_train, y_clf_test, y_reg_test):\n",
        "    super().__init__(X_train, X_test, y_clf_train, y_reg_train, y_clf_test, y_reg_test)\n",
        "    self.scaler = None\n",
        "    self.reg = None\n",
        "\n",
        "  def process_model(self):\n",
        "    # Scaling\n",
        "    self.scaler = StandardScaler()\n",
        "    X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
        "    X_test_scaled = self.scaler.transform(self.X_test)\n",
        "    return X_train_scaled, X_test_scaled\n",
        "\n",
        "  def train_model(self, X_train_scaled, X_test_scaled):\n",
        "    self.reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    self.reg.fit(X_train_scaled, self.y_reg_train)\n",
        "    return self.reg\n",
        "\n",
        "  def evaluate_model(self, X_test_scaled):\n",
        "    print(\"RMSE:\", mean_squared_error(self.y_reg_test, self.reg.predict(X_test_scaled)))\n",
        "\n",
        "  def run_model(self):\n",
        "    X_train_scaled, X_test_scaled = self.process_model()\n",
        "    model = self.train_model(X_train_scaled, X_test_scaled)\n",
        "    self.evaluate_model(X_test_scaled)\n",
        "    return self.scaler, model\n",
        "\n",
        "  def __del__(self):\n",
        "    del self.scaler\n",
        "    del self.reg"
      ],
      "metadata": {
        "id": "FTIdk7GmkAsz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class classification_model(Model):\n",
        "  def __init__(self, X_train, X_test, y_clf_train, y_reg_train, y_clf_test, y_reg_test):\n",
        "    super().__init__(X_train, X_test, y_clf_train, y_reg_train, y_clf_test, y_reg_test)\n",
        "    self.scaler = None\n",
        "    self.clf = None\n",
        "\n",
        "  def process_model(self):\n",
        "    # Scaling\n",
        "    self.scaler = StandardScaler()\n",
        "    X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
        "    X_test_scaled = self.scaler.transform(self.X_test)\n",
        "    return X_train_scaled, X_test_scaled\n",
        "\n",
        "  def train_model(self, X_train_scaled, X_test_scaled):\n",
        "    self.clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    self.clf.fit(X_train_scaled, self.y_clf_train)\n",
        "    return self.clf\n",
        "\n",
        "  def evaluate_model(self, X_test_scaled):\n",
        "    print(classification_report(self.y_clf_test, self.clf.predict(X_test_scaled)))\n",
        "\n",
        "  def run_model(self):\n",
        "    X_train_scaled, X_test_scaled = self.process_model()\n",
        "    model = self.train_model(X_train_scaled, X_test_scaled)\n",
        "    self.evaluate_model(X_test_scaled)\n",
        "    return model\n",
        "\n",
        "  def __del__(self):\n",
        "    del self.scaler\n",
        "    del self.clf"
      ],
      "metadata": {
        "id": "sKJTZwW8ld0m"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regMod = regression_model(X_train, X_test, y_clf_train, y_reg_train, y_clf_test, y_reg_test)\n",
        "scaler, regr = regMod.run_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8q8kiadl60s",
        "outputId": "e6acc02e-2a66-4e1e-b3a1-d74ba7886488"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 5582933414.570695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clsMod = classification_model(X_train, X_test, y_clf_train, y_reg_train, y_clf_test, y_reg_test)\n",
        "clf = clsMod.run_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W998XRhmmFxy",
        "outputId": "43355f45-f4f5-4f7c-d45e-f3fa7c30e754"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      1.00      0.94       222\n",
            "           1       0.00      0.00      0.00        28\n",
            "\n",
            "    accuracy                           0.89       250\n",
            "   macro avg       0.44      0.50      0.47       250\n",
            "weighted avg       0.79      0.89      0.84       250\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save everything\n",
        "joblib.dump({'scaler':scaler, 'clf':clf, 'reg':regr}, 'loan_risk_model.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRZgzWq9mNYa",
        "outputId": "1dcd7c8a-2d57-4dc3-cedf-b7bf19968792"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loan_risk_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2X1TpPtaBsYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aBXXgVYlDo9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from fastapi import FastAPI, HTTPException\n",
        "# from pydantic import BaseModel, Field\n",
        "# import pandas as pd\n",
        "# import joblib\n",
        "# from typing import Dict\n",
        "# import logging\n",
        "# import os\n",
        "# from pathlib import Path\n",
        "\n",
        "# # Configure logging\n",
        "# logging.basicConfig(level=logging.INFO)\n",
        "# logger = logging.getLogger(__name__)\n",
        "\n",
        "# app = FastAPI(title=\"Loan Risk Prediction API\")\n",
        "\n",
        "# # Load trained model artifacts with error handling\n",
        "# MODEL_PATH = Path(\"loan_risk_model.pkl\")\n",
        "# try:\n",
        "#     if not MODEL_PATH.exists():\n",
        "#         raise FileNotFoundError(f\"Model file {MODEL_PATH} not found\")\n",
        "#     model_bundle = joblib.load(MODEL_PATH)\n",
        "#     scaler = model_bundle.get(\"scaler\")\n",
        "#     clf = model_bundle.get(\"clf\")\n",
        "#     reg = model_bundle.get(\"reg\")\n",
        "#     if not all([scaler, clf, reg]):\n",
        "#         raise ValueError(\"Model bundle missing required keys: scaler, clf, reg\")\n",
        "# except Exception as e:\n",
        "#     logger.error(f\"Failed to load model: {str(e)}\")\n",
        "#     raise RuntimeError(f\"Model loading failed: {str(e)}\")\n",
        "\n",
        "# # In-memory storage (consider replacing with a database in production)\n",
        "# db: Dict[int, Dict] = {}\n",
        "# applicant_id_counter: int = 0\n",
        "\n",
        "# # Define input and response schemas\n",
        "# class Applicant(BaseModel):\n",
        "#     age: int = Field(..., ge=18, le=120, description=\"Applicant age\")\n",
        "#     income: float = Field(..., ge=0, description=\"Annual income in USD\")\n",
        "#     credit_score: float = Field(..., ge=300, le=850, description=\"Credit score\")\n",
        "#     loan_amount: float = Field(..., ge=0, description=\"Requested loan amount\")\n",
        "#     gender: str = Field(..., min_length=1, description=\"Gender\")\n",
        "#     purpose: str = Field(..., min_length=1, description=\"Loan purpose\")\n",
        "\n",
        "# class ApplicantResponse(BaseModel):\n",
        "#     applicant_id: int\n",
        "#     approved: bool\n",
        "#     default_risk_score: float\n",
        "#     input_data: Applicant\n",
        "\n",
        "# @app.post(\"/applicants/\", response_model=ApplicantResponse)\n",
        "# async def create_applicant(data: Applicant):\n",
        "#     global applicant_id_counter\n",
        "#     try:\n",
        "#         # Convert input to DataFrame and encode\n",
        "#         df = pd.DataFrame([data.dict()])\n",
        "#         df_encoded = pd.get_dummies(df)\n",
        "\n",
        "#         # Align columns with training set\n",
        "#         for col in scaler.feature_names_in_:\n",
        "#             if col not in df_encoded:\n",
        "#                 df_encoded[col] = 0\n",
        "#         df_encoded = df_encoded[scaler.feature_names_in_]\n",
        "\n",
        "#         # Scale and predict\n",
        "#         X_scaled = scaler.transform(df_encoded)\n",
        "#         prediction = bool(clf.predict(X_scaled)[0])\n",
        "#         risk_score = float(reg.predict(X_scaled)[0])\n",
        "\n",
        "#         # Store data\n",
        "#         applicant_id_counter += 1\n",
        "#         db[applicant_id_counter] = {\n",
        "#             \"input\": data.dict(),\n",
        "#             \"prediction\": prediction,\n",
        "#             \"risk_score\": risk_score\n",
        "#         }\n",
        "#         logger.info(f\"Created applicant ID {applicant_id_counter} with prediction: {prediction}\")\n",
        "#         return {\n",
        "#             \"applicant_id\": applicant_id_counter,\n",
        "#             \"approved\": prediction,\n",
        "#             \"default_risk_score\": risk_score,\n",
        "#             \"input_data\": data\n",
        "#         }\n",
        "#     except Exception as e:\n",
        "#         logger.error(f\"Error processing applicant: {str(e)}\")\n",
        "#         raise HTTPException(status_code=500, detail=f\"Prediction failed: {str(e)}\")\n",
        "\n",
        "# @app.get(\"/applicants/{applicant_id}\")\n",
        "# async def get_applicant(applicant_id: int):\n",
        "#     if applicant_id not in db:\n",
        "#         logger.warning(f\"Applicant ID {applicant_id} not found\")\n",
        "#         raise HTTPException(status_code=404, detail=\"Applicant not found\")\n",
        "#     logger.info(f\"Retrieved applicant ID {applicant_id}\")\n",
        "#     return db[applicant_id]\n",
        "\n",
        "# @app.put(\"/applicants/{applicant_id}\", response_model=ApplicantResponse)\n",
        "# async def update_applicant(applicant_id: int, data: Applicant):\n",
        "#     if applicant_id not in db:\n",
        "#         logger.warning(f\"Applicant ID {applicant_id} not found\")\n",
        "#         raise HTTPException(status_code=404, detail=\"Applicant not found\")\n",
        "#     try:\n",
        "#         # Re-predict with updated data\n",
        "#         df = pd.DataFrame([data.dict()])\n",
        "#         df_encoded = pd.get_dummies(df)\n",
        "#         for col in scaler.feature_names_in_:\n",
        "#             if col not in df_encoded:\n",
        "#                 df_encoded[col] = 0\n",
        "#         df_encoded = df_encoded[scaler.feature_names_in_]\n",
        "#         X_scaled = scaler.transform(df_encoded)\n",
        "#         prediction = bool(clf.predict(X_scaled)[0])\n",
        "#         risk_score = float(reg.predict(X_scaled)[0])\n",
        "\n",
        "#         # Update storage\n",
        "#         db[applicant_id] = {\n",
        "#             \"input\": data.dict(),\n",
        "#             \"prediction\": prediction,\n",
        "#             \"risk_score\": risk_score\n",
        "#         }\n",
        "#         logger.info(f\"Updated applicant ID {applicant_id} with prediction: {prediction}\")\n",
        "#         return {\n",
        "#             \"applicant_id\": applicant_id,\n",
        "#             \"approved\": prediction,\n",
        "#             \"default_risk_score\": risk_score,\n",
        "#             \"input_data\": data\n",
        "#         }\n",
        "#     except Exception as e:\n",
        "#         logger.error(f\"Error updating applicant {applicant_id}: {str(e)}\")\n",
        "#         raise HTTPException(status_code=500, detail=f\"Prediction failed: {str(e)}\")\n",
        "\n",
        "# @app.delete(\"/applicants/{applicant_id}\")\n",
        "# async def delete_applicant(applicant_id: int):\n",
        "#     if applicant_id not in db:\n",
        "#         logger.warning(f\"Applicant ID {applicant_id} not found\")\n",
        "#         raise HTTPException(status_code=404, detail=\"Applicant not found\")\n",
        "#     del db[applicant_id]\n",
        "#     logger.info(f\"Deleted applicant ID {applicant_id}\")\n",
        "#     return {\"deleted\": True}\n",
        "\n",
        "# # Health check endpoint\n",
        "# @app.get(\"/health\")\n",
        "# async def health_check():\n",
        "#     return {\"status\": \"healthy\"}"
      ],
      "metadata": {
        "id": "oBsbmJBODokL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# {\n",
        "#   \"age\": 30,\n",
        "#   \"income\": 50000.0,\n",
        "#   \"credit_score\": 700.0,\n",
        "#   \"loan_amount\": 10000.0,\n",
        "#   \"gender\": \"Male\",\n",
        "#   \"purpose\": \"Home\"\n",
        "# }"
      ],
      "metadata": {
        "id": "iR_nGylWOSDh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}